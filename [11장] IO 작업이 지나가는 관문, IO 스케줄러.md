사용자가 발생시키는 읽기, 쓰기와 같은 I/O 작업은 가상 파일 시스템, 로컬 파일 시스템 등의 경로를 거친 후 블록 디바이스로 전달되기 전에 I/O 스케줄러를 거치게 된다. I/O 스케줄러는 상대적으로 접근 속도가 느린 디스크에 대한 성능을 최대화하기 위해 구현된 커널의 일부분이며, 모든 I/O 작업은 이 I/O 스케줄러를 통해서 블록 디바이스(디스크)에 전달된다. I/O 스케줄러는 기본적으로 병합과 정렬이라는 두 가지 방식을 이용해서 I/O 요청을 블록 디바이스에 전달하게 되는데, 서버에서 발생하는 워크로드와 I/O 스케줄러의 알고리즘에 따라 성능을 더 좋게 만들 수도, 나쁘게 만들 수도 있다.

# 11.1 I/O 스케줄러의 필요성

I/O 스케줄러를 알아보기 전에 디스크에 대해 먼저 살펴보자. 디스크는 크게 두 종류로 나눌 수 있다. 헤드와 플래터 등의 기계식 부품으로 이루어진 하드 디스크 드라이브(HDD)와 플래시 메모리를 기반으로 이루어진 솔리드 스테이트 드라이브(SSD)이다.

HDD는 플래터(Platter)라는 원판과 같은 자기 장치가 있으며, 디스크 헤드가 플래터 위를 움직이면서 데이터를 읽거나 쓰는 작업을 한다. HDD에 저장되어 있는 데이터를 읽기 위해서는 디스크 헤드를 플래터의 특정 위치로 움직이게 해야 한다. 헤드는 기계 장치이기 때문에 시스템의 다른 부품들과 달리 이동하는 데에 시간이 걸리고 시스템을 구성하는 많은 부품들 중에서 가장 많은 시간이 소요된다. 그래서 헤드의 움직임을 최소화하고 한번 움직일 때 최대한 처리해야 I/O 성능이 극대화될 수 있다.

반면에 SSD는 플래시 메모리를 기반으로 구성되어 있다. SSD는 기계식 디스크와는 달리 헤드와 플래터 없이 기판에 장착되어 있는 플래시 메모리에 데이터를 쓰거나 읽는다. 헤드 대신에 컨트롤러라는 장치를 통해서 디스크로 유입되는 I/O 작업을 조정한다. SSD는 장치를 움직이지 않고 전기적 신호를 이용해서 접근하기 때문에 임의로 특정 섹터에 접근할 때 소요되는 시간이 모두 동일하다. 반면에 HDD는 특정 섹터에 접근하기 위해 헤드와 플래터를 움직여야 하기 때문에 현재 헤드의 위치가 어디냐에 따라 특정 영역에 접근하는 데 필요한 시간이 달라진다.

이렇게 디스크와 관련된 작업은 시간이 오래 소요되기 때문에 커널은 I/O 스케줄러를 통해서 조금이라도 성능을 극대화하려 한다. 그리고 이런 성능 극대화 작업을 위해서 병합과 정렬이라는 두 가지 방법을 사용한다.

먼저 병합을 살펴보자. **병합은 여러 개의 요청을 하나로 합치는 것을 의미한다.** 그림과 같은 경우를 가정해 보자. Request Queue에 총 3개의 요청이 인입되었는데, 첫 번째 요청은 10번 블록에서 1개의 블록 내용을 읽어오는 요청이고, 두 번째 요청은 11번 블록에서 1개의 블록 내용을 읽어오는 요청, 그리고 마지막 세 번째 요청은 12번 블록에서 1개의 블록 내용을 읽어오는 요청이다. 서로 다른 세 개의 요청이지만 접근 블록 주소가 인접해 있기 때문에 블록 디바이스의 Dispatch Queue에 3개의 요청을 넘겨주는 대신, 이 요청들을 하나의 큰 요청으로 합쳐서 넘겨주면 헤드의 움직임을 최소화할 수 있다. 10번 블록에 접근해서 3개의 블록만큼을 읽어오게 되면, 한 번의 요청으로 기존에 있던 3개의 I/O 요청을 처리할 수 있다. 이렇게 I/O 스케줄러는 병합을 통해서 디스크로의 명령 전달을 최소화하고 성능을 향상시킬 수 있다.

다음으로 정렬을 살펴보자. **정렬이란 여러 개의 요청을 섹터 순서대로 재배치하는 것을 의미한다.** 그림과 같은 상황을 가정해보자. I/O 요청이 총 4개가 들어왔다. 1번 블록, 7번 블록, 3번 블록, 10번 블록의 순서대로 들어온 상태에서 이 요청들을 정렬하지 않고 들어온 순서대로 처리하게 되면, 총 17만큼의 헤더 이동이 필요하다. 특히 7번 블록에서 3번 블록으로 갈 때는 앞으로 갔다가 다시 뒤로 가게 되어 불필요한 이동이 발생하게 된다. 하지만 이 요청을 들어온 순서대로가 아니라 블록 접근 순서대로 정렬한다면 헤더의 움직임이 어떻게 바뀔까?

정렬하지 않으면 총 17만큼의 이동이 필요하지만, 정렬하면 총 9만큼의 이동이 필요함을 확인할 수 있다. 헤더의 이동을 절반에 가깝게 줄일 수 있다. 이렇게 섹터 순서대로 최적의 경로를 갈 수 있도록 재배치하는 것을 정렬이라고 한다. 이렇게 되면 먼저 발생한 요청이 더 늦게 처리되는 문제가 생길 수도 있다. 그래서 I/O 스케줄러는 다양한 알고리즘으로 이런 문제를 해결하고 있다.

SSD의 경우는 어떨까? 헤드가 없기 때문에 헤드를 이동하느라 발생하는 비용이 없다. 그렇기 때문에 사실 정렬이 별 의미가 없다. 오히려 정렬하는데 CPU 리소스를 쓸데없이 사용하기 때문에 성능이 나빠질 수 있다. 그래서 SSD는 HDD와는 조금 다른 I/O 스케줄러를 사용한다.

# 11.2 I/O 스케줄러 설정

현재 시스템에 적용 가능한 I/O 스케줄러와 설정되어 있는 I/O 스케줄러 정보는 다음과 같이 확인할 수 있다.

```yaml
ubuntu@ip-172-31-0-62:~$ cd /sys/block/xvda/queue
ubuntu@ip-172-31-0-62:/sys/block/xvda/queue$ cat scheduler
none [mq-deadline]
```

`[]`로 표시되어 있는 부분이 현재 설정되어 있는 I/O 스케줄러이고, 그 외의 것들이 설정 가능한 I/O 스케줄러이다. 변경은 `echo` 명령을 이용하면 된다.

좋아 👌 아주 정확하게 잘 보고 있어.

지금 결과:

```
none [mq-deadline]
```

이걸 하나씩 해석해보자.

---

## 1️⃣ 대괄호 `[ ]` 의 의미

리눅스 블록 디바이스에서:

```
none [mq-deadline]
```

👉 **대괄호 안에 있는 게 현재 적용된 I/O 스케줄러**

즉,

> ✅ 현재 xvda에 적용된 스케줄러는 `mq-deadline`
> 

---

## 2️⃣ 그럼 앞에 `none`은 뭐야?

`none`은

> “스케줄링을 거의 하지 않는 모드”
> 

를 의미해.

즉 선택 가능한 스케줄러 목록이:

- none
- mq-deadline

이 두 개이고

현재는 `mq-deadline`이 선택되어 있는 상태야.

---

## 3️⃣ mq-deadline이 뭐냐?

`mq-deadline`은:

- multi-queue 환경에서 사용하는 Deadline 기반 스케줄러
- 요청에 deadline(기한)을 둬서 starvation 방지
- 읽기 요청을 우선 처리
- 예측 가능한 latency 제공

예전 single-queue deadline의 blk-mq 버전이라고 보면 돼.

---

## 4️⃣ 왜 bfq, kyber는 없을까?

EC2의 `xvda`는:

- Xen virtual block device
- 커널 설정 또는 디바이스 특성상
- 지원 스케줄러가 제한될 수 있음

그래서 현재 커널에서는

```
none
mq-deadline
```

만 제공하는 상태야.

---

## 5️⃣ 구조적으로 보면

현재 I/O 흐름은:

```
User write()
  ↓
VFS
  ↓
ext4
  ↓
page cache
  ↓
blk-mq
  ↓
mq-deadline (현재)
  ↓
xvda (Xen virtual disk)
  ↓
AWS storage backend
```

---

## 6️⃣ EC2에서 스케줄러 의미

EC2는:

- 실제 물리 디스크를 직접 제어하지 않음
- 하이퍼바이저가 중간에 존재
- AWS 스토리지 레이어가 자체적으로 스케줄링

그래서:

> 스케줄러 영향은 물리 서버보다 작음
> 

---

## 🎯 최종 해석 한 줄

```
none [mq-deadline]
```

👉 사용 가능한 스케줄러는 none, mq-deadline

👉 현재 적용된 것은 mq-deadline

---

지금 커널 I/O 경로 공부 중인 것 같은데 👀

다음 단계로 가볼까?

- 🔥 mq-deadline 내부 동작 방식
- 🔥 blk-mq에서 request가 hw queue로 내려가는 과정
- 🔥 Xen 가상 디스크에서 I/O가 실제 AWS 스토리지까지 가는 구조

어디까지 깊게 들어가볼까?

```yaml
root@ip-172-31-0-62:/sys/devices/vbd-768/block/xvda/queue# echo none > ./scheduler
root@ip-172-31-0-62:/sys/devices/vbd-768/block/xvda/queue# cat scheduler
[none] mq-deadline 
```

변경하면 재밌는 것을 볼 수 있는데, 각각의 I/O 스케줄러별로 튜닝 가능한 파라미터들이 바뀐다는 것이다. I/O 스케줄러를 변경하면 하위 디렉터리인 iosched 디렉터리에서도 값이 바뀐다.

```yaml
root@ip-172-31-0-62:/sys/devices/vbd-768/block/xvda/queue# cat scheduler
none [mq-deadline] 
root@ip-172-31-0-62:/sys/devices/vbd-768/block/xvda/queue# ls ./iosched
async_depth  fifo_batch  front_merges  prio_aging_expire  read_expire  write_expire  writes_starved
```

> none으로 변경하면 스케줄링을 하지 않으므로 iosched 디렉터리가 존재하지 않는다.
> 

# 11.3 cfq I/O 스케줄러

cfq는 Completely Fair Queueing의 약자로, 우리말로 하면 ‘완전 공정 큐잉’ I/O 스케줄러를 말한다. 공정하다는 단어에서 유추할 수 있듯이 **프로세스들이 발생시키는 I/O 요청들이 모든 프로세스에서 공정하게 실행되는 것이 특징이다.**

우선 각각의 프로세스에서 발생시킨 I/O는 Block I/O Layer를 거친 후 실제 디바이스로 내려가기 전에 cfq I/O 스케줄러를 거치게 된다. cfq I/O는 특성에 따라 각각 RT(Real Time), BE(Best Effort), IDLE 중 하나로 I/O 요청을 정의한다. 이 값은 `ionice` 명령을 이용해서 변경할 수 있으며, 대부분의 I/O 요청은 기본적으로 BE에 속한다. 위 형태는 I/O 처리의 우선순위를 설정하기 위해 나눈 것으로, RT에 속한 요청들을 가장 먼저 처리하고 IDLE에 속한 요청들을 가장 나중에 처리한다. I/O 우선순위에 따라 RT, BE, IDLE 셋 중 하나로 분류한 다음에는 service tree라 불리는 워크로드별 그룹으로 다시 나눈다. **SYNC는 순차적인 동기화 I/O 작업, 주로 순차 읽기**를 의미한다. **여기에 속한 큐들은 각각의 큐에 대한 처리를 완료한 후 일정 시간 동안 대기하게 된다.** **순차적인 작업이기 때문에 향후에 들어오는 I/O 요청도 현재 디스크 헤드와 가까운 위치의 작업이 들어올 확률이 높다.** 그래서 약간의 대기 시간이 있더라도 가까운 곳의 I/O 요청을 처리하게 되면 더 좋은 성능을 내는 데 도움이 되기 때문에 대기하게 된다. `slice_idle` 값으로 이 대기 시간을 설정할 수 있다. **SYNC_NOIDLE은 임의적인 동기화 I/O 작업, 주로 임의 읽기**를 의미한다. 여기에 속한 큐들은 SYNC 워크로드와는 달리 **각각의 큐에 대한 처리를 완료한 후 대기 시간 없이 바로 다음 큐에 대한 I/O 작업을 시작한다.** 임의 작업은 디스크 헤드를 많이 움직이게 하기 때문에 굳이 다음 요청을 기다려서 얻게 되는 성능상의 이점이 없기 때문이다. 그래서 기다리지 않는다는 뜻으로 NOIDLE이라는 이름을 붙인다. 마지막으로 **ASYNC는 비동기화 I/O 작업, 주로 쓰기 작업**을 의미한다. 이곳에는 **각각의 프로세스에서 발생하는 쓰기 작업이 같이 모여서 처리된다.** cfq는 프로세스별로 큐를 가지고 있지만 이 큐는 동기화 I/O 작업만을 처리하는 것이고, 비동기화 I/O 작업은 ASYNC 서비스 트리 밑에 하나로 모아놓고 한꺼번에 작업한다.

[ionice(1) - Linux manual page](https://man7.org/linux/man-pages/man1/ionice.1.html)

이렇게 I/O 우선순위와 워크로드에 따라 분류된 I/O 요청을 어떤 프로세스에서 발생시켰느냐에 따라 어떤 큐에 들어갈 것인지 최종적으로 정해진다. A 프로세스에서 발생시킨 요청은 cfq queue(A)에 속하게 되며, B 프로세스에서 발생시킨 요청은 cfq queue(B)에 속하게 된다. 만약 A 프로세스에서 쓰기 요청을 발생시켰다면 cfg queue (A)에 속하지 않고 ASYNC 서비스 트리 밑에 만들어진 큐에 들어가게 된다. cfq I/O 스케줄러는 이렇게 나뉜 I/O 요청들을 cfg queue에 넣고 각각 동등한 time slice를 할당한 다음 이 값을 기준으로 큐들을 순차적으로 처리한다. 그래서 **cfg I/O 스케줄러는 모든 프로세스들에 치우침 없이 동등한 I/O 요청 처리 기회를 주지만, 일부 I/O를 많이 일으키거나 더 빨리 처리되어야 하는 I/O를 가진 프로세스들의 경우에 자신의 차례가 될 때까지 기다려야 하기 때문에 I/O 요청에 대한 성능이 낮아질 가능성이 있다.**