사용자가 발생시키는 읽기, 쓰기와 같은 I/O 작업은 가상 파일 시스템, 로컬 파일 시스템 등의 경로를 거친 후 블록 디바이스로 전달되기 전에 I/O 스케줄러를 거치게 된다. I/O 스케줄러는 상대적으로 접근 속도가 느린 디스크에 대한 성능을 최대화하기 위해 구현된 커널의 일부분이며, 모든 I/O 작업은 이 I/O 스케줄러를 통해서 블록 디바이스(디스크)에 전달된다. I/O 스케줄러는 기본적으로 병합과 정렬이라는 두 가지 방식을 이용해서 I/O 요청을 블록 디바이스에 전달하게 되는데, 서버에서 발생하는 워크로드와 I/O 스케줄러의 알고리즘에 따라 성능을 더 좋게 만들 수도, 나쁘게 만들 수도 있다.

# 11.1 I/O 스케줄러의 필요성

I/O 스케줄러를 알아보기 전에 디스크에 대해 먼저 살펴보자. 디스크는 크게 두 종류로 나눌 수 있다. 헤드와 플래터 등의 기계식 부품으로 이루어진 하드 디스크 드라이브(HDD)와 플래시 메모리를 기반으로 이루어진 솔리드 스테이트 드라이브(SSD)이다.

HDD는 플래터(Platter)라는 원판과 같은 자기 장치가 있으며, 디스크 헤드가 플래터 위를 움직이면서 데이터를 읽거나 쓰는 작업을 한다. HDD에 저장되어 있는 데이터를 읽기 위해서는 디스크 헤드를 플래터의 특정 위치로 움직이게 해야 한다. 헤드는 기계 장치이기 때문에 시스템의 다른 부품들과 달리 이동하는 데에 시간이 걸리고 시스템을 구성하는 많은 부품들 중에서 가장 많은 시간이 소요된다. 그래서 헤드의 움직임을 최소화하고 한번 움직일 때 최대한 처리해야 I/O 성능이 극대화될 수 있다.

반면에 SSD는 플래시 메모리를 기반으로 구성되어 있다. SSD는 기계식 디스크와는 달리 헤드와 플래터 없이 기판에 장착되어 있는 플래시 메모리에 데이터를 쓰거나 읽는다. 헤드 대신에 컨트롤러라는 장치를 통해서 디스크로 유입되는 I/O 작업을 조정한다. SSD는 장치를 움직이지 않고 전기적 신호를 이용해서 접근하기 때문에 임의로 특정 섹터에 접근할 때 소요되는 시간이 모두 동일하다. 반면에 HDD는 특정 섹터에 접근하기 위해 헤드와 플래터를 움직여야 하기 때문에 현재 헤드의 위치가 어디냐에 따라 특정 영역에 접근하는 데 필요한 시간이 달라진다.

이렇게 디스크와 관련된 작업은 시간이 오래 소요되기 때문에 커널은 I/O 스케줄러를 통해서 조금이라도 성능을 극대화하려 한다. 그리고 이런 성능 극대화 작업을 위해서 병합과 정렬이라는 두 가지 방법을 사용한다.

먼저 병합을 살펴보자. **병합은 여러 개의 요청을 하나로 합치는 것을 의미한다.** 그림과 같은 경우를 가정해 보자. Request Queue에 총 3개의 요청이 인입되었는데, 첫 번째 요청은 10번 블록에서 1개의 블록 내용을 읽어오는 요청이고, 두 번째 요청은 11번 블록에서 1개의 블록 내용을 읽어오는 요청, 그리고 마지막 세 번째 요청은 12번 블록에서 1개의 블록 내용을 읽어오는 요청이다. 서로 다른 세 개의 요청이지만 접근 블록 주소가 인접해 있기 때문에 블록 디바이스의 Dispatch Queue에 3개의 요청을 넘겨주는 대신, 이 요청들을 하나의 큰 요청으로 합쳐서 넘겨주면 헤드의 움직임을 최소화할 수 있다. 10번 블록에 접근해서 3개의 블록만큼을 읽어오게 되면, 한 번의 요청으로 기존에 있던 3개의 I/O 요청을 처리할 수 있다. 이렇게 I/O 스케줄러는 병합을 통해서 디스크로의 명령 전달을 최소화하고 성능을 향상시킬 수 있다.

다음으로 정렬을 살펴보자. **정렬이란 여러 개의 요청을 섹터 순서대로 재배치하는 것을 의미한다.** 그림과 같은 상황을 가정해보자. I/O 요청이 총 4개가 들어왔다. 1번 블록, 7번 블록, 3번 블록, 10번 블록의 순서대로 들어온 상태에서 이 요청들을 정렬하지 않고 들어온 순서대로 처리하게 되면, 총 17만큼의 헤더 이동이 필요하다. 특히 7번 블록에서 3번 블록으로 갈 때는 앞으로 갔다가 다시 뒤로 가게 되어 불필요한 이동이 발생하게 된다. 하지만 이 요청을 들어온 순서대로가 아니라 블록 접근 순서대로 정렬한다면 헤더의 움직임이 어떻게 바뀔까?

정렬하지 않으면 총 17만큼의 이동이 필요하지만, 정렬하면 총 9만큼의 이동이 필요함을 확인할 수 있다. 헤더의 이동을 절반에 가깝게 줄일 수 있다. 이렇게 섹터 순서대로 최적의 경로를 갈 수 있도록 재배치하는 것을 정렬이라고 한다. 이렇게 되면 먼저 발생한 요청이 더 늦게 처리되는 문제가 생길 수도 있다. 그래서 I/O 스케줄러는 다양한 알고리즘으로 이런 문제를 해결하고 있다.

SSD의 경우는 어떨까? 헤드가 없기 때문에 헤드를 이동하느라 발생하는 비용이 없다. 그렇기 때문에 사실 정렬이 별 의미가 없다. 오히려 정렬하는데 CPU 리소스를 쓸데없이 사용하기 때문에 성능이 나빠질 수 있다. 그래서 SSD는 HDD와는 조금 다른 I/O 스케줄러를 사용한다.

# 11.2 I/O 스케줄러 설정

현재 시스템에 적용 가능한 I/O 스케줄러와 설정되어 있는 I/O 스케줄러 정보는 다음과 같이 확인할 수 있다.

```yaml
ubuntu@ip-172-31-0-62:~$ cd /sys/block/xvda/queue
ubuntu@ip-172-31-0-62:/sys/block/xvda/queue$ cat scheduler
none [mq-deadline]
```

`[]`로 표시되어 있는 부분이 현재 설정되어 있는 I/O 스케줄러이고, 그 외의 것들이 설정 가능한 I/O 스케줄러이다. 변경은 `echo` 명령을 이용하면 된다.

좋아 👌 아주 정확하게 잘 보고 있어.

지금 결과:

```
none [mq-deadline]
```

이걸 하나씩 해석해보자.

---

## 1️⃣ 대괄호 `[ ]` 의 의미

리눅스 블록 디바이스에서:

```
none [mq-deadline]
```

👉 **대괄호 안에 있는 게 현재 적용된 I/O 스케줄러**

즉,

> ✅ 현재 xvda에 적용된 스케줄러는 `mq-deadline`
> 

---

## 2️⃣ 그럼 앞에 `none`은 뭐야?

`none`은

> “스케줄링을 거의 하지 않는 모드”
> 

를 의미해.

즉 선택 가능한 스케줄러 목록이:

- none
- mq-deadline

이 두 개이고

현재는 `mq-deadline`이 선택되어 있는 상태야.

---

## 3️⃣ mq-deadline이 뭐냐?

`mq-deadline`은:

- multi-queue 환경에서 사용하는 Deadline 기반 스케줄러
- 요청에 deadline(기한)을 둬서 starvation 방지
- 읽기 요청을 우선 처리
- 예측 가능한 latency 제공

예전 single-queue deadline의 blk-mq 버전이라고 보면 돼.

---

## 4️⃣ 왜 bfq, kyber는 없을까?

EC2의 `xvda`는:

- Xen virtual block device
- 커널 설정 또는 디바이스 특성상
- 지원 스케줄러가 제한될 수 있음

그래서 현재 커널에서는

```
none
mq-deadline
```

만 제공하는 상태야.

---

## 5️⃣ 구조적으로 보면

현재 I/O 흐름은:

```
User write()
  ↓
VFS
  ↓
ext4
  ↓
page cache
  ↓
blk-mq
  ↓
mq-deadline (현재)
  ↓
xvda (Xen virtual disk)
  ↓
AWS storage backend
```

---

## 6️⃣ EC2에서 스케줄러 의미

EC2는:

- 실제 물리 디스크를 직접 제어하지 않음
- 하이퍼바이저가 중간에 존재
- AWS 스토리지 레이어가 자체적으로 스케줄링

그래서:

> 스케줄러 영향은 물리 서버보다 작음
> 

---

## 🎯 최종 해석 한 줄

```
none [mq-deadline]
```

👉 사용 가능한 스케줄러는 none, mq-deadline

👉 현재 적용된 것은 mq-deadline

---

지금 커널 I/O 경로 공부 중인 것 같은데 👀

다음 단계로 가볼까?

- 🔥 mq-deadline 내부 동작 방식
- 🔥 blk-mq에서 request가 hw queue로 내려가는 과정
- 🔥 Xen 가상 디스크에서 I/O가 실제 AWS 스토리지까지 가는 구조

어디까지 깊게 들어가볼까?

```yaml
root@ip-172-31-0-62:/sys/devices/vbd-768/block/xvda/queue# echo none > ./scheduler
root@ip-172-31-0-62:/sys/devices/vbd-768/block/xvda/queue# cat scheduler
[none] mq-deadline 
```

변경하면 재밌는 것을 볼 수 있는데, 각각의 I/O 스케줄러별로 튜닝 가능한 파라미터들이 바뀐다는 것이다. I/O 스케줄러를 변경하면 하위 디렉터리인 iosched 디렉터리에서도 값이 바뀐다.

```yaml
root@ip-172-31-0-62:/sys/devices/vbd-768/block/xvda/queue# cat scheduler
none [mq-deadline] 
root@ip-172-31-0-62:/sys/devices/vbd-768/block/xvda/queue# ls ./iosched
async_depth  fifo_batch  front_merges  prio_aging_expire  read_expire  write_expire  writes_starved
```

> none으로 변경하면 스케줄링을 하지 않으므로 iosched 디렉터리가 존재하지 않는다.
> 

# 11.3 cfq I/O 스케줄러

cfq는 Completely Fair Queueing의 약자로, 우리말로 하면 ‘완전 공정 큐잉’ I/O 스케줄러를 말한다. 공정하다는 단어에서 유추할 수 있듯이 **프로세스들이 발생시키는 I/O 요청들이 모든 프로세스에서 공정하게 실행되는 것이 특징이다.**

우선 각각의 프로세스에서 발생시킨 I/O는 Block I/O Layer를 거친 후 실제 디바이스로 내려가기 전에 cfq I/O 스케줄러를 거치게 된다. cfq I/O는 특성에 따라 각각 RT(Real Time), BE(Best Effort), IDLE 중 하나로 I/O 요청을 정의한다. 이 값은 `ionice` 명령을 이용해서 변경할 수 있으며, 대부분의 I/O 요청은 기본적으로 BE에 속한다. 위 형태는 I/O 처리의 우선순위를 설정하기 위해 나눈 것으로, RT에 속한 요청들을 가장 먼저 처리하고 IDLE에 속한 요청들을 가장 나중에 처리한다. I/O 우선순위에 따라 RT, BE, IDLE 셋 중 하나로 분류한 다음에는 service tree라 불리는 워크로드별 그룹으로 다시 나눈다. **SYNC는 순차적인 동기화 I/O 작업, 주로 순차 읽기**를 의미한다. **여기에 속한 큐들은 각각의 큐에 대한 처리를 완료한 후 일정 시간 동안 대기하게 된다.** **순차적인 작업이기 때문에 향후에 들어오는 I/O 요청도 현재 디스크 헤드와 가까운 위치의 작업이 들어올 확률이 높다.** 그래서 약간의 대기 시간이 있더라도 가까운 곳의 I/O 요청을 처리하게 되면 더 좋은 성능을 내는 데 도움이 되기 때문에 대기하게 된다. `slice_idle` 값으로 이 대기 시간을 설정할 수 있다. **SYNC_NOIDLE은 임의적인 동기화 I/O 작업, 주로 임의 읽기**를 의미한다. 여기에 속한 큐들은 SYNC 워크로드와는 달리 **각각의 큐에 대한 처리를 완료한 후 대기 시간 없이 바로 다음 큐에 대한 I/O 작업을 시작한다.** 임의 작업은 디스크 헤드를 많이 움직이게 하기 때문에 굳이 다음 요청을 기다려서 얻게 되는 성능상의 이점이 없기 때문이다. 그래서 기다리지 않는다는 뜻으로 NOIDLE이라는 이름을 붙인다. 마지막으로 **ASYNC는 비동기화 I/O 작업, 주로 쓰기 작업**을 의미한다. 이곳에는 **각각의 프로세스에서 발생하는 쓰기 작업이 같이 모여서 처리된다.** cfq는 프로세스별로 큐를 가지고 있지만 이 큐는 동기화 I/O 작업만을 처리하는 것이고, 비동기화 I/O 작업은 ASYNC 서비스 트리 밑에 하나로 모아놓고 한꺼번에 작업한다.

[ionice(1) - Linux manual page](https://man7.org/linux/man-pages/man1/ionice.1.html)

이렇게 I/O 우선순위와 워크로드에 따라 분류된 I/O 요청을 어떤 프로세스에서 발생시켰느냐에 따라 어떤 큐에 들어갈 것인지 최종적으로 정해진다. A 프로세스에서 발생시킨 요청은 cfq queue(A)에 속하게 되며, B 프로세스에서 발생시킨 요청은 cfq queue(B)에 속하게 된다. 만약 A 프로세스에서 쓰기 요청을 발생시켰다면 cfg queue (A)에 속하지 않고 ASYNC 서비스 트리 밑에 만들어진 큐에 들어가게 된다. cfq I/O 스케줄러는 이렇게 나뉜 I/O 요청들을 cfg queue에 넣고 각각 동등한 time slice를 할당한 다음 이 값을 기준으로 큐들을 순차적으로 처리한다. 그래서 **cfg I/O 스케줄러는 모든 프로세스들에 치우침 없이 동등한 I/O 요청 처리 기회를 주지만, 일부 I/O를 많이 일으키거나 더 빨리 처리되어야 하는 I/O를 가진 프로세스들의 경우에 자신의 차례가 될 때까지 기다려야 하기 때문에 I/O 요청에 대한 성능이 낮아질 가능성이 있다.**

첫번째 값은 `back_seek_max`이다. 이 값은 현재 디스크의 헤드가 위치한 곳을 기준으로 backward seeking(섹터 번호가 감소하는 방향, 디스크 섹터가 뒤로 이동하는 것)의 최댓값을 의미한다. backward seking 값의 기준 안에 들어오는 요청은 바로 다음 요청으로 간주되어 처리된다. 예를 들어 현재 헤드의 위치를 10이라고 하고, `back_seek_max`값을 5라고 가정한다면 헤드의 위치 10에서 5를 뺀 섹터 5번 ~ 9번까지의 요청들이 바로 다음 요청으로 간주되어 처리된다. 이 값은 인접한 섹터의 요청이 들어왔을 때 바로 처리할 수 있게 되어 헤더의 움직임을 최소화하는 데 도움이 되지만, 그만큼 다른 요청들이 밀릴 수 있기 때문에 적당한 값을 유지하는 것이 좋다. 만약 시스템에 순차 쓰기가 주를 이룬다면 이 값을 줄이는게 도움이 된다.

두번째 값은 `back_seek_penalty`이다. `back_seek_max`와 비슷하지만 이 값은 backward seeking의 패널티를 정의한다. 헤더를 움직이는 비용은 매우 크기 때문에 가능하다면 순서대로 움직이는 것이 좋다. 즉 1→2→3과 같은 식으로 계속해서 증가하는 추세로 동작하는 것이 1→3→2→4와 같이 뒤로 가는 것보다 성능에 더 도움이 된다. 그렇기 때문에 뒤로 갈 때는 `back_seek_penalty` 값으로 패널티를 적용한다. 예를 들어 현재 헤더의 위치가 10이고, 5번 섹터로 가는 요청과 15번 섹터로 가는 요청이 있다고 가정해보자. 둘 다 현재 헤더를 기준으로 5만큼 이동하겠지만 cfq는 여기에 `back_seek_penalty` 값을 곱해서 서로의 거리를 다르게 본다. 즉, 똑같이 5만큼 이동하지만 5번 섹터로 가는 것이 더 무겁고 성능에 좋지 않은 영향을 주는 작업이라고 인지하는 것이다.

세 번째 값은 `fifo_expire_async`이다. cfq에도 시간을 기준으로 한 fifo 리스트가 존재하고 그 중에서 async 요청에 대한 만료 시간을 정의한다. **async는 비동기적이라는 의미로, 주로 쓰기 요청을 나타낸다.** 애플리케이션의 쓰기 작업은 커널에 의해서 dirty page 쓰기 작업만으로 끝날 수 있다. 실제 디스크로의 쓰기 요청까지 발생하지 않고 메모리에 쓰기 작업을 하는 것만으로도 쓰기 작업이 완료되었다고 끝낼 수 있으며, 그렇기 때문에 대부분의 쓰기 작업은 애플리케이션을 블록시키지 않고 비동기적으로 완료된다. 이 값은 비동기적인 I/O 작업에 대한 만료 시간을 정의한다.

네번째 값은 `fifo_expire_sync` 값이다. `fifo_expire_async` 값과 유사하지만 sync 요청에 대한 만료 시간을 의미한다. **sync는 동기적이라는 의미로, 주로 읽기 요청을 나타낸다.** 만약 애플리케이션이 특정 파일의 내용을 읽는 작업을 한다면, 대부분의 경우는 읽기가 완료되어야만 다음 작업을 진행할 수 있다. 예를 들어 애플리케이션이 환경 변수가 설정되어 있는 설정 파일을 읽는다고 가정했을 때, 일부분만 읽어서는 애플리케이션이 동작할 수 없고 전체를 읽어 들여야 동작할 수 있다. 그래서 읽기 동작이 완료될 때까지 애플리케이션을 블록시킨다. 이 값은 동기적인 I/O 작업에 대한 만료 시간을 정의한다.

다섯번째 값은 `group_idle`이다. cfq는 원래 프로세스별로 할당된 큐를 이동할 때 해당 큐에 할당된 시간을 전부 사용하지 않았지만 I/O 요청이 전부 처리되어 이동해야 한다면 해당 큐는 idle 상태가 된다. 당장은 I/O 요청이 없지만 혹시라도 추가로 발생하게 될지 모르기 때문이다. 이 과정 중에서도 `group_idle`이 설정되어 있다면 같은 cgroup 안에 포함된 프로세스들에 대해서는 큐를 이동할 때 기다리지 않고 바로 다음 큐로 넘어가도록 동작한다. 하지만 같은 cgroup 안에 I.O 요청을 처리하고 다른 cgroup으로 넘어가려고 할 때에는 `group_idle`에 정의된 시간만큼을 대기한다.

여섯 번째는 `group_isolation`이다. 이 값 역시 cgroup과 관련이 있다. 단어가 의미하는 것처럼 서로 다른 cgroup 간의 차이를 더 명확하게 해주는 역할을 한다. 기본값은 0이며, 이 값이 0이 되면 위에서 언급한 서비스 트리 중 SYNC_NOIDLE에 속하는 큐들을 cgorup의 루트 그룹으로 이동시킨다. SYNC_NOIDLE은 임의 접근이기 때문에 디스크 헤드를 많이 움직이게 되고, 순차 접근에 비해 I/O 요청 완료까지 걸리는 시간이 상대적으로 오래 걸린다. 만약 각각의 cgroup 별로 SYNC_NOIDLE을 처리해야 한다면, SYNC_NOIDLE이 많은 경우 cgroup 별로 처리해야 하기 때문에 전체적인 성능이 낮아진다. **cgroup 별로 임의 접근을 위한 SYNC_NOIDLE 서비스 트리를 만들지 않고 시스템의 모든 임의 접근을 하나의 SYNC_NOIDLE 서비스 트리에 모아놓고 한 번에 처리하면 I/O 성능이 더 좋아진다.** 그리고 **이때 임의 접근을 루트 cgroup에 있는 SYNC_NOIDLE에 모은다.** 하지만 이렇게 되면 **cgroup별로 설정한 값이 임의 접근에는 적용되지 않는다.** 이때 group_isolation을 1로 설정하면 **임의 접근도 cgroup 값의 영향을 받게 되고 각각의 cgroup 분류가 더 명확해진다.** 따라서 이 값은 cgroup을 활용하는 환경에서는 성능에 큰 영향을 미치는 중요한 값이다.